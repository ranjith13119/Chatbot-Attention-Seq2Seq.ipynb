{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chatbot-Attention-Seq2Seq.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1tyKwhKP615cydTF6NAeJ1a5eeSfg2AFb",
      "authorship_tag": "ABX9TyNpDb2xqFHIVC3sxFsK1gfF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ranjith13119/Chatbot-Attention-Seq2Seq.ipynb/blob/main/Chatbot_Attention_Seq2Seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F24A01HuDrAH"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkiWnODPFtL5"
      },
      "source": [
        "lines = open('/content/drive/MyDrive/NLP/Chatbot/movie_lines.txt', encoding = 'utf-8', errors = 'ignore').read().split('\\n')\n",
        "conversations = open('/content/drive/MyDrive/NLP/Chatbot/movie_conversations.txt', encoding = 'utf-8', errors = 'ignore').read().split('\\n')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgUdATkGGJ5X"
      },
      "source": [
        "exchn = []\n",
        "for conver in conversations:\n",
        "  exchn.append(conver.split(' +++$+++ ')[-1][1:-1].replace(\"'\", \" \").replace(\",\",\"\").split())"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wiu8IXk6G2JV"
      },
      "source": [
        "diag = {}\n",
        "for line in lines:\n",
        "    diag[line.split(' +++$+++ ')[0]] = line.split(' +++$+++ ')[-1]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Up_PRnbmHqqF"
      },
      "source": [
        "questions = [] # ---> Input\n",
        "answers = []  # --> Target"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ik90Dmm4Hsp7"
      },
      "source": [
        "for conver in exchn:\n",
        "    for i in range(len(conver) - 1):\n",
        "        questions.append(diag[conver[i]]) # THe first one is the Question id\n",
        "        answers.append(diag[conver[i+1]]) # the next line item is the answer id"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Iz2DiJeH-OD"
      },
      "source": [
        "sorted_ques = []\n",
        "sorted_ans = []\n",
        "for i in range(len(questions)):\n",
        "    if len(questions[i]) < 20:  # taking small \n",
        "        sorted_ques.append(questions[i])\n",
        "        sorted_ans.append(answers[i])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rb7Ezm3pJuaN"
      },
      "source": [
        "def clean_text(txt):\n",
        "    txt = txt.lower()\n",
        "    txt = re.sub(r\"i'm\", \"i am\", txt)\n",
        "    txt = re.sub(r\"he's\", \"he is\", txt)\n",
        "    txt = re.sub(r\"she's\", \"she is\", txt)\n",
        "    txt = re.sub(r\"that's\", \"that is\", txt)\n",
        "    txt = re.sub(r\"what's\", \"what is\", txt)\n",
        "    txt = re.sub(r\"where's\", \"where is\", txt)\n",
        "    txt = re.sub(r\"\\'ll\", \" will\", txt)\n",
        "    txt = re.sub(r\"\\'ve\", \" have\", txt)\n",
        "    txt = re.sub(r\"\\'re\", \" are\", txt)\n",
        "    txt = re.sub(r\"\\'d\", \" would\", txt)\n",
        "    txt = re.sub(r\"won't\", \"will not\", txt)\n",
        "    txt = re.sub(r\"can't\", \"can not\", txt)\n",
        "    txt = re.sub(r\"[^\\w\\s]\", \"\", txt)\n",
        "    return txt"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5LRdDyIJueV"
      },
      "source": [
        "clean_ques = []\n",
        "clean_ans = []"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOnwrEo-Jug9"
      },
      "source": [
        "import re\n",
        "for line in sorted_ques:\n",
        "    clean_ques.append(clean_text(line))\n",
        "        \n",
        "for line in sorted_ans:\n",
        "    clean_ans.append(clean_text(line))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KLM5oi_Jul8"
      },
      "source": [
        "for i in range(len(clean_ans)):\n",
        "    clean_ans[i] = ' '.join(clean_ans[i].split()[:20])"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bSIKJqxTjQj"
      },
      "source": [
        "clean_ans=clean_ans[:30000]\n",
        "clean_ques=clean_ques[:30000]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XebwQ84DKulM"
      },
      "source": [
        "word2count = {}\n",
        "\n",
        "for line in clean_ques:\n",
        "    for word in line.split():\n",
        "        if word not in word2count:\n",
        "            word2count[word] = 1\n",
        "        else:\n",
        "            word2count[word] += 1 \n",
        "for line in clean_ans:\n",
        "    for word in line.split():\n",
        "        if word not in word2count:\n",
        "            word2count[word] = 1\n",
        "        else:\n",
        "            word2count[word] += 1"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLpqNSyrKuxT"
      },
      "source": [
        "# remove the word which is having less than 10% occurances\n",
        "thresh = 5\n",
        "\n",
        "vocab = {}\n",
        "word_num = 0\n",
        "for word, count in word2count.items():\n",
        "    if count >= thresh:\n",
        "        vocab[word] = word_num\n",
        "        word_num += 1        "
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCCTjWMUKu6T"
      },
      "source": [
        "for i in range(len(clean_ans)):\n",
        "    clean_ans[i] = '<SOS> ' + clean_ans[i] + ' <EOS>'"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0HgsdOFK7Wn"
      },
      "source": [
        "tokens = ['<PAD>', '<EOS>', '<OUT>', '<SOS>']\n",
        "x = len(vocab)\n",
        "for token in tokens:\n",
        "    vocab[token] = x\n",
        "    x += 1"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7X9X4kcLGkS"
      },
      "source": [
        "vocab['cameron'] = vocab['<PAD>']\n",
        "vocab['<PAD>'] = 0"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cr15TRlKLGp_"
      },
      "source": [
        "### inv answers dict ###\n",
        "inv_vocab = {w:v for v, w in vocab.items()}"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGBvPvByLGr4"
      },
      "source": [
        "# check whether the word is precent in the vocab. Else assign the word as <OUT>\n",
        "encoder_inp = []\n",
        "for line in clean_ques:\n",
        "    lst = []\n",
        "    for word in line.split():\n",
        "        if word not in vocab:\n",
        "            lst.append(vocab['<OUT>'])\n",
        "        else:\n",
        "            lst.append(vocab[word])\n",
        "        \n",
        "    encoder_inp.append(lst)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7IPpAt0LUBr"
      },
      "source": [
        "decoder_inp = []\n",
        "for line in clean_ans:\n",
        "    lst = []\n",
        "    for word in line.split():\n",
        "        if word not in vocab:\n",
        "            lst.append(vocab['<OUT>'])\n",
        "        else:\n",
        "            lst.append(vocab[word])        \n",
        "    decoder_inp.append(lst)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2QqkZ7RLXwU"
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "encoder_inp = pad_sequences(encoder_inp, 20, padding='post', truncating='post')\n",
        "decoder_inp = pad_sequences(decoder_inp, 20, padding='post', truncating='post')"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhCBij5aLbeh"
      },
      "source": [
        "decoder_final_output = []\n",
        "for i in decoder_inp:\n",
        "    decoder_final_output.append(i[1:]) \n",
        "\n",
        "decoder_final_output = pad_sequences(decoder_final_output, 20, padding='post', truncating='post')"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jS9jOuEiLbiE",
        "outputId": "d8b315f9-f492-4550-b0b6-d07e1201d682"
      },
      "source": [
        "VOCAB_SIZE = len(vocab)\n",
        "MAX_LEN = 20\n",
        "\n",
        "print(decoder_final_output.shape, decoder_inp.shape, encoder_inp.shape, len(vocab), len(inv_vocab), inv_vocab[0])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(30000, 20) (30000, 20) (30000, 20) 3734 3733 <PAD>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9L53_c_qL-Rq",
        "outputId": "9f7af311-d2ca-4181-90e3-f3d65d40ce9a"
      },
      "source": [
        "decoder_final_output"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  22,  357,    5, ...,  549,   13,    0],\n",
              "       [3731, 1782,    4, ...,   46,    4,    0],\n",
              "       [  42, 3730,    0, ...,    0,    0,    0],\n",
              "       ...,\n",
              "       [ 147,  375, 1608, ...,    0,    0,    0],\n",
              "       [3730,    0,    0, ...,    0,    0,    0],\n",
              "       [3730,    0,    0, ...,    0,    0,    0]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FigXHJUiLboD"
      },
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "decoder_final_output = to_categorical(decoder_final_output, len(vocab))"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdTenAMcMERj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3aeae01-f586-4197-fb9a-51045286b944"
      },
      "source": [
        "decoder_final_output.shape"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(30000, 20, 3734)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQfwC8iMMEUF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70de7909-7807-4a9c-f217-c3b8da270e54"
      },
      "source": [
        "embeddings_index = {}\n",
        "with open('/content/drive/MyDrive/NLP/unsupervised Text summary/glove.6B.50d.txt', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "    f.close()\n",
        "\n",
        "print(\"Glove Loded!\")"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Glove Loded!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TpnPThiVMEWx"
      },
      "source": [
        "embedding_dimention = 50\n",
        "def embedding_matrix_creater(embedding_dimention, word_index):\n",
        "    embedding_matrix = np.zeros((len(word_index)+1, embedding_dimention))\n",
        "    for word, i in word_index.items():\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "          # words not found in embedding index will be all-zeros.\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "    return embedding_matrix\n",
        "embedding_matrix = embedding_matrix_creater(50, word_index=vocab)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjcRf4YBMEZZ"
      },
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, Input, Bidirectional, Concatenate, Dropout, Attention"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqEr8KayMEbV"
      },
      "source": [
        "embed = Embedding(VOCAB_SIZE+1, \n",
        "                  50,                   \n",
        "                  input_length=20,\n",
        "                  trainable=True)\n",
        "\n",
        "embed.build((None,))\n",
        "embed.set_weights([embedding_matrix])"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1qRbBSDMdaV"
      },
      "source": [
        "enc_inp = Input(shape=(20, ))"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "USEeHOxlMroH"
      },
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "from tensorflow.python.keras.layers import Layer\n",
        "from tensorflow.python.keras import backend as K\n",
        "\n",
        "\n",
        "class AttentionLayer(Layer):\n",
        "    \"\"\"\n",
        "      This class implements Bahdanau attention (https://arxiv.org/pdf/1409.0473.pdf).\n",
        "      There are three sets of weights introduced W_a, U_a, and V_a\n",
        "     \"\"\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(AttentionLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert isinstance(input_shape, list)\n",
        "        # Create a trainable weight variable for this layer.\n",
        "\n",
        "        self.W_a = self.add_weight(name='W_a',\n",
        "                                   shape=tf.TensorShape((input_shape[0][2], input_shape[0][2])),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True) # for decoder stats\n",
        "        self.U_a = self.add_weight(name='U_a',\n",
        "                                   shape=tf.TensorShape((input_shape[1][2], input_shape[0][2])),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)  # for encoder output\n",
        "        self.V_a = self.add_weight(name='V_a',\n",
        "                                   shape=tf.TensorShape((input_shape[0][2], 1)),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)  # fully connected layer\n",
        "\n",
        "        super(AttentionLayer, self).build(input_shape)  # Be sure to call this at the end\n",
        "\n",
        "    def call(self, inputs, verbose=False):\n",
        "        \"\"\"\n",
        "          inputs: [encoder_output_sequence, decoder_output_sequence]\n",
        "        \"\"\"\n",
        "        assert type(inputs) == list\n",
        "        encoder_out_seq, decoder_out_seq = inputs\n",
        "        if verbose:\n",
        "            print('encoder_out_seq>', encoder_out_seq.shape)\n",
        "            print('decoder_out_seq>', decoder_out_seq.shape)\n",
        "\n",
        "        def energy_step(inputs, states):\n",
        "            \"\"\"              \n",
        "              Step function for computing energy for a single decoder state\n",
        "              inputs: (batchsize * 1 * de_in_dim)\n",
        "              states: (batchsize * 1 * de_latent_dim)\n",
        "            \"\"\"\n",
        "\n",
        "            assert_msg = \"States must be an iterable. Got {} of type {}\".format(states, type(states))\n",
        "            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n",
        "\n",
        "            \"\"\" Some parameters required for shaping tensors\"\"\"\n",
        "            en_seq_len, en_hidden = encoder_out_seq.shape[1], encoder_out_seq.shape[2]\n",
        "            de_hidden = inputs.shape[-1]\n",
        "\n",
        "            \"\"\" Computing S.Wa where S=[s0, s1, ..., si]\"\"\"\n",
        "            # <= batch size * en_seq_len * latent_dim\n",
        "            W_a_dot_s = K.dot(encoder_out_seq, self.W_a)\n",
        "\n",
        "            \"\"\" Computing hj.Ua \"\"\"\n",
        "            U_a_dot_h = K.expand_dims(K.dot(inputs, self.U_a), 1)  # <= batch_size, 1, latent_dim\n",
        "            if verbose:\n",
        "                print('Ua.h>', U_a_dot_h.shape)\n",
        "\n",
        "            \"\"\" tanh(S.Wa + hj.Ua) \"\"\"\n",
        "            # <= batch_size*en_seq_len, latent_dim\n",
        "            Ws_plus_Uh = K.tanh(W_a_dot_s + U_a_dot_h)\n",
        "            if verbose:\n",
        "                print('Ws+Uh>', Ws_plus_Uh.shape)\n",
        "\n",
        "            \"\"\" softmax(va.tanh(S.Wa + hj.Ua)) \"\"\"\n",
        "            # <= batch_size, en_seq_len\n",
        "            e_i = K.squeeze(K.dot(Ws_plus_Uh, self.V_a), axis=-1)\n",
        "            # <= batch_size, en_seq_len\n",
        "            e_i = K.softmax(e_i)\n",
        "\n",
        "            if verbose:\n",
        "                print('ei>', e_i.shape)\n",
        "\n",
        "            return e_i, [e_i]\n",
        "\n",
        "        def context_step(inputs, states):\n",
        "            \"\"\" Step function for computing ci using ei \"\"\"\n",
        "\n",
        "            assert_msg = \"States must be an iterable. Got {} of type {}\".format(states, type(states))\n",
        "            assert isinstance(states, list) or isinstance(states, tuple), assert_msg\n",
        "\n",
        "            # <= batch_size, hidden_size\n",
        "            c_i = K.sum(encoder_out_seq * K.expand_dims(inputs, -1), axis=1)\n",
        "            if verbose:\n",
        "                print('ci>', c_i.shape)\n",
        "            return c_i, [c_i]\n",
        "\n",
        "        fake_state_c = K.sum(encoder_out_seq, axis=1)\n",
        "        fake_state_e = K.sum(encoder_out_seq, axis=2)  # <= (batch_size, enc_seq_len, latent_dim\n",
        "\n",
        "        \"\"\" Computing energy outputs \"\"\"\n",
        "        # e_outputs => (batch_size, de_seq_len, en_seq_len)\n",
        "        last_out, e_outputs, _ = K.rnn(\n",
        "            energy_step, decoder_out_seq, [fake_state_e],\n",
        "        )\n",
        "\n",
        "        \"\"\" Computing context vectors \"\"\"\n",
        "        last_out, c_outputs, _ = K.rnn(\n",
        "            context_step, e_outputs, [fake_state_c],\n",
        "        )\n",
        "\n",
        "        return c_outputs, e_outputs\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        \"\"\" Outputs produced by the layer \"\"\"\n",
        "        return [\n",
        "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[1][2])),\n",
        "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[0][1]))\n",
        "        ]"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5418X6rMgkk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8888528-bc78-4217-be32-28a1c5f11b47"
      },
      "source": [
        "enc_embed = embed(enc_inp)\n",
        "enc_lstm = Bidirectional(LSTM(400, return_state=True, dropout=0.05, return_sequences = True))\n",
        "\n",
        "encoder_outputs, forward_h, forward_c, backward_h, backward_c = enc_lstm(enc_embed)\n",
        "\n",
        "state_h = Concatenate()([forward_h, backward_h])\n",
        "state_c = Concatenate()([forward_c, backward_c])\n",
        "\n",
        "enc_states = [state_h, state_c]\n",
        "\n",
        "dec_inp = Input(shape=(20, ))\n",
        "dec_embed = embed(dec_inp)\n",
        "dec_lstm = LSTM(400*2, return_state=True, return_sequences=True, dropout=0.05)\n",
        "output, _, _ = dec_lstm(dec_embed, initial_state=enc_states)\n",
        "\n",
        "# attention\n",
        "attn_layer = AttentionLayer()\n",
        "attn_op, attn_state = attn_layer([encoder_outputs, output])\n",
        "decoder_concat_input = Concatenate(axis=-1)([output, attn_op])\n",
        "\n",
        "\n",
        "dec_dense = Dense(VOCAB_SIZE, activation='softmax')\n",
        "final_output = dec_dense(decoder_concat_input)\n",
        "\n",
        "model = Model([enc_inp, dec_inp], final_output)\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_2 (InputLayer)            [(None, 20)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_1 (InputLayer)            [(None, 20)]         0                                            \n",
            "__________________________________________________________________________________________________\n",
            "embedding (Embedding)           (None, 20, 50)       186750      input_1[0][0]                    \n",
            "                                                                 input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "bidirectional (Bidirectional)   [(None, 20, 800), (N 1443200     embedding[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 800)          0           bidirectional[0][1]              \n",
            "                                                                 bidirectional[0][3]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 800)          0           bidirectional[0][2]              \n",
            "                                                                 bidirectional[0][4]              \n",
            "__________________________________________________________________________________________________\n",
            "lstm_1 (LSTM)                   [(None, 20, 800), (N 2723200     embedding[1][0]                  \n",
            "                                                                 concatenate[0][0]                \n",
            "                                                                 concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "attention_layer (AttentionLayer ((None, 20, 800), (N 1280800     bidirectional[0][0]              \n",
            "                                                                 lstm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 20, 1600)     0           lstm_1[0][0]                     \n",
            "                                                                 attention_layer[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 20, 3734)     5978134     concatenate_2[0][0]              \n",
            "==================================================================================================\n",
            "Total params: 11,612,084\n",
            "Trainable params: 11,612,084\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHnlOKkFNB5S"
      },
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdgY54JKNB7q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "315b8875-3f0b-42f0-9e6a-7641dc0b1947"
      },
      "source": [
        "model.fit([encoder_inp, decoder_inp], decoder_final_output, epochs=10, batch_size=24, validation_split=0.15) # with minimal "
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1063/1063 [==============================] - 169s 148ms/step - loss: 2.4287 - acc: 0.5983 - val_loss: 2.1187 - val_acc: 0.6367\n",
            "Epoch 2/10\n",
            "1063/1063 [==============================] - 155s 146ms/step - loss: 2.1185 - acc: 0.6220 - val_loss: 2.0439 - val_acc: 0.6439\n",
            "Epoch 3/10\n",
            "1063/1063 [==============================] - 155s 146ms/step - loss: 1.9877 - acc: 0.6293 - val_loss: 2.0269 - val_acc: 0.6451\n",
            "Epoch 4/10\n",
            "1063/1063 [==============================] - 157s 147ms/step - loss: 1.8546 - acc: 0.6358 - val_loss: 2.0404 - val_acc: 0.6482\n",
            "Epoch 5/10\n",
            "1063/1063 [==============================] - 157s 148ms/step - loss: 1.6953 - acc: 0.6473 - val_loss: 2.0841 - val_acc: 0.6472\n",
            "Epoch 6/10\n",
            "1063/1063 [==============================] - 157s 148ms/step - loss: 1.5161 - acc: 0.6675 - val_loss: 2.1378 - val_acc: 0.6444\n",
            "Epoch 7/10\n",
            "1063/1063 [==============================] - 157s 148ms/step - loss: 1.3373 - acc: 0.6949 - val_loss: 2.1974 - val_acc: 0.6439\n",
            "Epoch 8/10\n",
            "1063/1063 [==============================] - 157s 148ms/step - loss: 1.1725 - acc: 0.7239 - val_loss: 2.2703 - val_acc: 0.6403\n",
            "Epoch 9/10\n",
            "1063/1063 [==============================] - 157s 148ms/step - loss: 1.0287 - acc: 0.7531 - val_loss: 2.3450 - val_acc: 0.6378\n",
            "Epoch 10/10\n",
            "1063/1063 [==============================] - 156s 147ms/step - loss: 0.9014 - acc: 0.7812 - val_loss: 2.4231 - val_acc: 0.6366\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f389795a2d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_sf8icHNB_E"
      },
      "source": [
        "model.save('chatbot.h5')\n",
        "model.save_weights('chatbot_weights.h5')"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COQV19j3NWXT"
      },
      "source": [
        "enc_model = tf.keras.models.Model(enc_inp, [encoder_outputs, enc_states])\n",
        "\n",
        "\n",
        "decoder_state_input_h = tf.keras.layers.Input(shape=( 400 * 2,))\n",
        "decoder_state_input_c = tf.keras.layers.Input(shape=( 400 * 2,))\n",
        "\n",
        "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
        "\n",
        "\n",
        "decoder_outputs, state_h, state_c = dec_lstm(dec_embed , initial_state=decoder_states_inputs)\n",
        "\n",
        "\n",
        "decoder_states = [state_h, state_c]\n",
        "\n",
        "#decoder_output = dec_dense(decoder_outputs)\n",
        "\n",
        "dec_model = tf.keras.models.Model([dec_inp, decoder_states_inputs],\n",
        "                                      [decoder_outputs] + decoder_states)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8aHMjZ_NWaJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb47326a-1de0-42a8-ac7f-bda3617e88a4"
      },
      "source": [
        "print(\"##########################################\")\n",
        "print(\"#       start chatting ver. 1.0          #\")\n",
        "print(\"##########################################\")\n",
        "\n",
        "\n",
        "prepro1 = \"\"\n",
        "while prepro1 != 'q':\n",
        "    \n",
        "    prepro1 = input(\"you : \")\n",
        "    prepro = [prepro1]\n",
        "    \n",
        "    try:\n",
        "        txt = []\n",
        "        for x in prepro:\n",
        "            lst = []\n",
        "            for y in x.split():\n",
        "                lst.append(vocab[y])\n",
        "            txt.append(lst)\n",
        "        txt = pad_sequences(txt, 20, padding='post')\n",
        "\n",
        "\n",
        "        ###\n",
        "        enc_op, stat = enc_model.predict( txt ) \n",
        "\n",
        "        empty_target_seq = np.zeros( ( 1 , 1) )\n",
        "        empty_target_seq[0, 0] = vocab['<SOS>']\n",
        "        stop_condition = False\n",
        "        decoded_translation = ''\n",
        "\n",
        "\n",
        "        while not stop_condition :\n",
        "\n",
        "            dec_outputs , h , c = dec_model.predict([ empty_target_seq ] + stat )\n",
        "\n",
        "            ###\n",
        "            ###########################\n",
        "            attn_op, attn_state = attn_layer([enc_op, dec_outputs])\n",
        "            decoder_concat_input = Concatenate(axis=-1)([dec_outputs, attn_op])\n",
        "            decoder_concat_input = dec_dense(decoder_concat_input)\n",
        "            ###########################\n",
        "\n",
        "            sampled_word_index = np.argmax( decoder_concat_input[0, -1, :] )\n",
        "\n",
        "            sampled_word = inv_vocab[sampled_word_index] + ' '\n",
        "\n",
        "            if sampled_word != '<EOS> ':\n",
        "                decoded_translation += sampled_word           \n",
        "\n",
        "\n",
        "            if sampled_word == '<EOS> ' or len(decoded_translation.split()) > 13:\n",
        "                stop_condition = True\n",
        "\n",
        "            empty_target_seq = np.zeros( ( 1 , 1 ) )  \n",
        "            empty_target_seq[ 0 , 0 ] = sampled_word_index\n",
        "            stat = [ h , c ] \n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    print(\"chatbot attention : \", decoded_translation )\n",
        "    print(\"==============================================\")"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "##########################################\n",
            "#       start chatting ver. 1.0          #\n",
            "##########################################\n",
            "you : hey\n",
            "chatbot attention :  hey \n",
            "==============================================\n",
            "you : i love you\n",
            "chatbot attention :  i love you \n",
            "==============================================\n",
            "you : how are you\n",
            "chatbot attention :  insane i am just wondering how much how do you feel about \n",
            "==============================================\n",
            "you : thanks\n",
            "chatbot attention :  you are welcome \n",
            "==============================================\n",
            "you : are you back at party\n",
            "chatbot attention :  no \n",
            "==============================================\n",
            "you : when will you back\n",
            "chatbot attention :  i will be in the morning \n",
            "==============================================\n",
            "you : I haven't seen you for two weeks\n",
            "chatbot attention :  i will be in the morning \n",
            "==============================================\n",
            "you : what's that make you\n",
            "chatbot attention :  i will be in the morning \n",
            "==============================================\n",
            "you : i hate you\n",
            "chatbot attention :  i know \n",
            "==============================================\n",
            "you : how do you knwo\n",
            "chatbot attention :  i know \n",
            "==============================================\n",
            "you : how?\n",
            "chatbot attention :  i know \n",
            "==============================================\n",
            "you : ok..you are boring...bye\n",
            "chatbot attention :  i know \n",
            "==============================================\n",
            "you : bye\n",
            "chatbot attention :  bye \n",
            "==============================================\n",
            "you : thanks\n",
            "chatbot attention :  you are welcome \n",
            "==============================================\n",
            "you : hey\n",
            "chatbot attention :  hey \n",
            "==============================================\n",
            "you : how are you/\n",
            "chatbot attention :  hey \n",
            "==============================================\n",
            "you : how are you\n",
            "chatbot attention :  insane i am just wondering how much how do you feel about \n",
            "==============================================\n",
            "you : i love you\n",
            "chatbot attention :  i love you \n",
            "==============================================\n",
            "you : you are boring\n",
            "chatbot attention :  i am not \n",
            "==============================================\n",
            "you : yes you are\n",
            "chatbot attention :  i am tired \n",
            "==============================================\n",
            "you : ok bye\n",
            "chatbot attention :  bye \n",
            "==============================================\n",
            "you : when will you come from party\n",
            "chatbot attention :  i will not \n",
            "==============================================\n",
            "you : i have not seen you for passed three days\n",
            "chatbot attention :  you are not \n",
            "==============================================\n",
            "you : i have not seen you for past three weeks\n",
            "chatbot attention :  you are sorry arent you \n",
            "==============================================\n",
            "you : I have not seen you for past two weeks\n",
            "chatbot attention :  you are sorry arent you \n",
            "==============================================\n",
            "you :  what's that make you\n",
            "chatbot attention :  you are sorry arent you \n",
            "==============================================\n",
            "you : i haven't seen you for pass two weeks\n",
            "chatbot attention :  you are sorry arent you \n",
            "==============================================\n",
            "you : Well, there's someone I think might be\n",
            "chatbot attention :  you are sorry arent you \n",
            "==============================================\n",
            "you : I really don't think I need any social advice from you right now\n",
            "chatbot attention :  you are sorry arent you \n",
            "==============================================\n",
            "you : can you give me some advice\n",
            "chatbot attention :  i am wearing a <OUT> \n",
            "==============================================\n",
            "you : what's that mek you\n",
            "chatbot attention :  i am wearing a <OUT> \n",
            "==============================================\n",
            "you : what's that make you\n",
            "chatbot attention :  i am wearing a <OUT> \n",
            "==============================================\n",
            "you : q\n",
            "chatbot attention :  i am wearing a <OUT> \n",
            "==============================================\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}